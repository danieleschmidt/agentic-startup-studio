# =============================================================================
# Enhanced Prometheus Alerting Rules for Agentic Startup Studio
# Comprehensive monitoring for application, infrastructure, and business metrics
# =============================================================================

groups:
  # ==========================================================================
  # Application Health & Performance Alerts
  # ==========================================================================
  - name: application.health
    interval: 15s
    rules:
      - alert: ApplicationDown
        expr: up{job="startup-studio-api"} == 0
        for: 30s
        labels:
          severity: critical
          team: platform
          category: availability
        annotations:
          summary: "Application is down"
          description: "The Agentic Startup Studio API has been down for more than 30 seconds"
          runbook_url: "https://docs.terragonlabs.com/runbooks/application-down"
          
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="startup-studio-api"}[5m])) > 2
        for: 2m
        labels:
          severity: warning
          team: platform
          category: performance
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s for the last 2 minutes"
          runbook_url: "https://docs.terragonlabs.com/runbooks/high-response-time"
          
      - alert: HighErrorRate
        expr: (rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])) * 100 > 5
        for: 1m
        labels:
          severity: critical
          team: platform
          category: reliability
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% for the last 1 minute"
          runbook_url: "https://docs.terragonlabs.com/runbooks/high-error-rate"

  # ==========================================================================
  # Infrastructure Alerts
  # ==========================================================================
  - name: infrastructure.resources
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: (100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          category: resources
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}"
          
      - alert: HighMemoryUsage
        expr: ((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * 100 > 90
        for: 3m
        labels:
          severity: critical
          team: infrastructure
          category: resources
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 90% for more than 3 minutes on {{ $labels.instance }}"
          
      - alert: DiskSpaceLow
        expr: ((node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          category: storage
        annotations:
          summary: "Disk space running low"
          description: "Disk usage is above 85% on {{ $labels.device }} at {{ $labels.instance }}"

  # ==========================================================================
  # Database Alerts
  # ==========================================================================
  - name: database.health
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          team: data
          category: availability
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 1 minute"
          runbook_url: "https://docs.terragonlabs.com/runbooks/postgresql-down"
          
      - alert: HighDatabaseConnections
        expr: (pg_stat_database_numbackends / pg_settings_max_connections) * 100 > 80
        for: 2m
        labels:
          severity: warning
          team: data
          category: performance
        annotations:
          summary: "High database connection usage"
          description: "Database connection usage is above 80% for more than 2 minutes"
          
      - alert: SlowQueries
        expr: rate(pg_stat_statements_mean_time_ms[5m]) > 1000
        for: 3m
        labels:
          severity: warning
          team: data
          category: performance
        annotations:
          summary: "Slow database queries detected"
          description: "Average query time is {{ $value }}ms for the last 3 minutes"

  # ==========================================================================
  # Business Logic Alerts
  # ==========================================================================
  - name: business.pipeline
    interval: 60s
    rules:
      - alert: IdeaProcessingStalled
        expr: increase(pipeline_ideas_processed_total[10m]) == 0
        for: 10m
        labels:
          severity: warning
          team: product
          category: business
        annotations:
          summary: "Idea processing has stalled"
          description: "No ideas have been processed in the last 10 minutes"
          runbook_url: "https://docs.terragonlabs.com/runbooks/idea-processing-stalled"
          
      - alert: HighBudgetConsumption
        expr: (budget_consumed_dollars / budget_total_dollars) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: finance
          category: cost
        annotations:
          summary: "High budget consumption"
          description: "Budget consumption is above 80% of the allocated limit"
          
      - alert: CriticalBudgetConsumption
        expr: (budget_consumed_dollars / budget_total_dollars) * 100 > 95
        for: 1m
        labels:
          severity: critical
          team: finance
          category: cost
        annotations:
          summary: "Critical budget consumption"
          description: "Budget consumption is above 95% of the allocated limit"

  # ==========================================================================
  # Security Alerts
  # ==========================================================================
  - name: security.monitoring
    interval: 30s
    rules:
      - alert: UnauthorizedAccess
        expr: rate(http_requests_total{status="401"}[5m]) > 10
        for: 1m
        labels:
          severity: warning
          team: security
          category: access
        annotations:
          summary: "High rate of unauthorized access attempts"
          description: "More than 10 unauthorized access attempts per second for 1 minute"
          
      - alert: SuspiciousActivity
        expr: rate(http_requests_total{status="403"}[5m]) > 5
        for: 2m
        labels:
          severity: warning
          team: security
          category: access
        annotations:
          summary: "Suspicious activity detected"
          description: "High rate of forbidden access attempts detected"
          
      - alert: RateLimitExceeded
        expr: rate(http_requests_total{status="429"}[5m]) > 20
        for: 30s
        labels:
          severity: info
          team: platform
          category: ratelimit
        annotations:
          summary: "Rate limit frequently exceeded"
          description: "Rate limit is being exceeded frequently, consider adjusting limits"

  # ==========================================================================
  # External Dependencies Alerts
  # ==========================================================================
  - name: external.dependencies
    interval: 60s
    rules:
      - alert: OpenAIAPIDown
        expr: external_api_success_rate{service="openai"} < 0.95
        for: 2m
        labels:
          severity: warning
          team: platform
          category: dependencies
        annotations:
          summary: "OpenAI API experiencing issues"
          description: "OpenAI API success rate is below 95% for 2 minutes"
          
      - alert: HighExternalAPILatency
        expr: external_api_response_time_seconds{quantile="0.95"} > 5
        for: 3m
        labels:
          severity: warning
          team: platform
          category: performance
        annotations:
          summary: "High external API latency"
          description: "95th percentile external API response time is above 5 seconds"

  # ==========================================================================
  # Data Quality Alerts
  # ==========================================================================
  - name: data.quality
    interval: 120s
    rules:
      - alert: DataValidationFailures
        expr: rate(data_validation_failures_total[10m]) > 0.1
        for: 5m
        labels:
          severity: warning
          team: data
          category: quality
        annotations:
          summary: "High data validation failure rate"
          description: "Data validation failure rate is above 10% for 5 minutes"
          
      - alert: DuplicateDetectionIssues
        expr: duplicate_detection_accuracy < 0.9
        for: 5m
        labels:
          severity: warning
          team: data
          category: quality
        annotations:
          summary: "Duplicate detection accuracy degraded"
          description: "Duplicate detection accuracy is below 90%"

  # ==========================================================================
  # SLO-Based Alerts (AI/ML Specific)
  # ==========================================================================
  - name: slo.ai_agents
    interval: 30s
    rules:
      - alert: AIAgentSLOBreach
        expr: |
          (
            sum(rate(ai_agent_operations_total{status="success"}[5m])) by (agent_type) /
            sum(rate(ai_agent_operations_total[5m])) by (agent_type)
          ) < 0.95
        for: 5m
        labels:
          severity: critical
          team: ai-ml
          category: slo
          slo: "ai_agent_success_rate"
        annotations:
          summary: "AI Agent SLO breach detected"
          description: "AI Agent {{ $labels.agent_type }} success rate {{ $value }} is below SLO target of 95%"
          runbook_url: "https://docs.terragonlabs.com/runbooks/ai-agent-slo-breach"
          
      - alert: AIAgentLatencySLOBreach
        expr: |
          histogram_quantile(0.95,
            rate(ai_agent_processing_duration_seconds_bucket[5m])
          ) > 30
        for: 3m
        labels:
          severity: warning
          team: ai-ml
          category: slo
          slo: "ai_agent_latency"
        annotations:
          summary: "AI Agent latency SLO breach"
          description: "AI Agent 95th percentile latency {{ $value }}s exceeds 30s SLO target"
          
      - alert: TokenEfficiencySLOBreach
        expr: |
          (
            sum(rate(ai_tokens_consumed_total[5m])) by (agent_type) /
            sum(rate(ai_agent_operations_total{status="success"}[5m])) by (agent_type)
          ) > 2000
        for: 10m
        labels:
          severity: warning
          team: ai-ml
          category: slo
          slo: "token_efficiency"
        annotations:
          summary: "Token efficiency SLO breach"
          description: "AI Agent {{ $labels.agent_type }} using {{ $value }} tokens per operation, exceeding target of 2000"

  # ==========================================================================
  # Business KPI SLO Alerts
  # ==========================================================================
  - name: slo.business_kpis
    interval: 60s
    rules:
      - alert: IdeaGenerationRateSLOBreach
        expr: rate(startup_ideas_generated_total[1h]) < 10
        for: 15m
        labels:
          severity: warning
          team: product
          category: slo
          slo: "idea_generation_rate"
        annotations:
          summary: "Idea generation rate below SLO"
          description: "Generating {{ $value }} ideas per hour, below target of 10/hour"
          business_impact: "medium"
          
      - alert: MVPDeploymentSLOBreach
        expr: |
          (
            sum(rate(mvp_deployments_successful_total[1h])) /
            sum(rate(mvp_deployments_attempted_total[1h]))
          ) < 0.85
        for: 10m
        labels:
          severity: critical
          team: platform
          category: slo
          slo: "mvp_deployment_success"
        annotations:
          summary: "MVP deployment success rate below SLO"
          description: "MVP deployment success rate {{ $value }} is below 85% target"
          business_impact: "high"
          
      - alert: CostPerIdeaSLOBreach
        expr: |
          (
            sum(rate(api_cost_usd_total[1h])) /
            sum(rate(startup_ideas_generated_total[1h]))
          ) > 2.0
        for: 10m
        labels:
          severity: warning
          team: finance
          category: slo
          slo: "cost_per_idea"
        annotations:
          summary: "Cost per idea exceeds SLO target"
          description: "Cost per idea is ${{ $value }}, exceeding $2.00 target"
          cost_impact: "high"

  # ==========================================================================
  # Error Budget Alerts
  # ==========================================================================
  - name: slo.error_budgets
    interval: 300s  # Check every 5 minutes
    rules:
      - alert: ErrorBudgetExhausted
        expr: |
          (
            1 - (
              sum(rate(http_requests_total{code!~"5.."}[30d])) /
              sum(rate(http_requests_total[30d]))
            )
          ) > (1 - 0.999)
        for: 0s  # Alert immediately
        labels:
          severity: critical
          team: sre
          category: error_budget
          slo: "api_availability"
        annotations:
          summary: "API availability error budget exhausted"
          description: "30-day error budget for API availability (99.9% SLO) has been exhausted"
          action_required: "immediate"
          
      - alert: ErrorBudgetCriticallyLow
        expr: |
          (
            (1 - 0.999) - (
              1 - (
                sum(rate(http_requests_total{code!~"5.."}[30d])) /
                sum(rate(http_requests_total[30d]))
              )
            )
          ) / (1 - 0.999) < 0.10
        for: 0s
        labels:
          severity: warning
          team: sre
          category: error_budget
          slo: "api_availability"
        annotations:
          summary: "API availability error budget critically low"
          description: "Only {{ $value | humanizePercentage }} of 30-day error budget remaining"
          
      - alert: AIAgentErrorBudgetLow
        expr: |
          (
            0.05 - (
              1 - (
                sum(rate(ai_agent_operations_total{status="success"}[24h])) /
                sum(rate(ai_agent_operations_total[24h]))
              )
            )
          ) / 0.05 < 0.25
        for: 0s
        labels:
          severity: warning
          team: ai-ml
          category: error_budget
          slo: "ai_agent_success"
        annotations:
          summary: "AI Agent error budget low"
          description: "AI Agent 24-hour error budget is at {{ $value | humanizePercentage }}"

  # ==========================================================================
  # Multi-Burn Rate Alerts (Advanced SLO Alerting)
  # ==========================================================================
  - name: slo.multi_burn_rate
    interval: 30s
    rules:
      # Fast burn rate (2% of budget in 1 hour)
      - alert: APIAvailabilityFastBurn
        expr: |
          (
            sum(rate(http_requests_total{code=~"5.."}[1m])) /
            sum(rate(http_requests_total[1m]))
          ) > (14.4 * (1 - 0.999))
          and
          (
            sum(rate(http_requests_total{code=~"5.."}[5m])) /
            sum(rate(http_requests_total[5m]))
          ) > (14.4 * (1 - 0.999))
        for: 2m
        labels:
          severity: critical
          team: sre
          category: burn_rate
          slo: "api_availability"
          burn_rate: "fast"
        annotations:
          summary: "Fast burn rate detected for API availability"
          description: "Current error rate will exhaust 2% of monthly error budget in 1 hour"
          
      # Slow burn rate (10% of budget in 6 hours)
      - alert: APIAvailabilitySlowBurn
        expr: |
          (
            sum(rate(http_requests_total{code=~"5.."}[1h])) /
            sum(rate(http_requests_total[1h]))
          ) > (6 * (1 - 0.999))
          and
          (
            sum(rate(http_requests_total{code=~"5.."}[6h])) /
            sum(rate(http_requests_total[6h]))
          ) > (6 * (1 - 0.999))
        for: 15m
        labels:
          severity: warning
          team: sre
          category: burn_rate
          slo: "api_availability"
          burn_rate: "slow"
        annotations:
          summary: "Slow burn rate detected for API availability"
          description: "Current error rate will exhaust 10% of monthly error budget in 6 hours"