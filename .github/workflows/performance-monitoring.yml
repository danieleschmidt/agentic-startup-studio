name: Performance Monitoring

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - api
          - database
          - pipeline

env:
  PYTHON_VERSION: '3.11'

jobs:
  performance-baseline:
    name: Performance Baseline
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: perf_test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install --system -r requirements.txt
          uv pip install --system -e ".[dev,test]"
          uv pip install --system pytest-benchmark locust

      - name: Initialize database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/perf_test_db
        run: |
          python -c "
from pipeline.storage.idea_repository import IdeaRepository
from pipeline.config.settings import get_settings
import asyncio

async def setup():
    settings = get_settings()
    repo = IdeaRepository(settings)
    await repo.initialize_db()

asyncio.run(setup())
"

      - name: Run API performance tests
        if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'api' || github.event.inputs.benchmark_type == ''
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/perf_test_db
          REDIS_URL: redis://localhost:6379
        run: |
          pytest tests/performance/ -m "api" \
            --benchmark-json=api-benchmark.json \
            --benchmark-only \
            --benchmark-min-rounds=5 \
            --benchmark-warmup=on

      - name: Run database performance tests
        if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'database' || github.event.inputs.benchmark_type == ''
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/perf_test_db
        run: |
          pytest tests/performance/ -m "database" \
            --benchmark-json=db-benchmark.json \
            --benchmark-only \
            --benchmark-min-rounds=10 \
            --benchmark-warmup=on

      - name: Run pipeline performance tests
        if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'pipeline' || github.event.inputs.benchmark_type == ''
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/perf_test_db
          REDIS_URL: redis://localhost:6379
        run: |
          pytest tests/performance/ -m "pipeline" \
            --benchmark-json=pipeline-benchmark.json \
            --benchmark-only \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=on

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: |
            *-benchmark.json

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    timeout-minutes: 20

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: load_test_db
        ports:
          - 5432:5432

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install uv locust
          uv pip install --system -r requirements.txt
          uv pip install --system -e .

      - name: Start API server
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/load_test_db
        run: |
          python scripts/serve_api.py --port 8000 &
          sleep 10
          curl -f http://localhost:8000/health || exit 1

      - name: Run load tests
        run: |
          locust -f tests/performance/load-test.py \
            --host=http://localhost:8000 \
            --users=50 \
            --spawn-rate=5 \
            --run-time=2m \
            --html=load-test-report.html \
            --csv=load-test-results \
            --headless

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test-results*.csv

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: memory_test_db
        ports:
          - 5432:5432

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install uv memory-profiler psutil
          uv pip install --system -r requirements.txt
          uv pip install --system -e .

      - name: Run memory profiling
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/memory_test_db
        run: |
          python -m memory_profiler scripts/performance_benchmark.py > memory-profile.txt

      - name: Generate memory report
        run: |
          echo "# Memory Usage Report" > memory-report.md
          echo "" >> memory-report.md
          echo "## Peak Memory Usage" >> memory-report.md
          echo '```' >> memory-report.md
          cat memory-profile.txt >> memory-report.md
          echo '```' >> memory-report.md

      - name: Upload memory profile
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile
          path: |
            memory-profile.txt
            memory-report.md

  performance-regression:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: [performance-baseline]
    if: github.event_name == 'pull_request'
    timeout-minutes: 15

    steps:
      - name: Checkout current
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: current-benchmarks/

      - name: Checkout main branch
        uses: actions/checkout@v4
        with:
          ref: main
          path: main-branch/

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Compare performance
        run: |
          python3 -c "
import json
import os

def load_benchmark(filepath):
    if not os.path.exists(filepath):
        return None
    with open(filepath, 'r') as f:
        return json.load(f)

current_api = load_benchmark('current-benchmarks/api-benchmark.json')
current_db = load_benchmark('current-benchmarks/db-benchmark.json')

print('# Performance Regression Analysis')
print('')

if current_api:
    print('## API Performance')
    for benchmark in current_api.get('benchmarks', []):
        name = benchmark['name']
        mean = benchmark['stats']['mean']
        print(f'- {name}: {mean:.4f}s')
    print('')

if current_db:
    print('## Database Performance') 
    for benchmark in current_db.get('benchmarks', []):
        name = benchmark['name']
        mean = benchmark['stats']['mean']
        print(f'- {name}: {mean:.4f}s')

print('')
print('⚠️  Baseline comparison requires historical data.')
print('Consider implementing performance trend tracking.')
" > performance-comparison.md

      - name: Upload performance comparison
        uses: actions/upload-artifact@v4
        with:
          name: performance-comparison
          path: performance-comparison.md

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comparison = fs.readFileSync('performance-comparison.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comparison
            });

  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [performance-baseline, load-testing, memory-profiling]
    if: always()

    steps:
      - name: Generate performance summary
        run: |
          echo "# Performance Monitoring Summary - $(date)" > performance-summary.md
          echo "" >> performance-summary.md
          
          echo "## Test Results" >> performance-summary.md
          echo "- Performance Baseline: ${{ needs.performance-baseline.result }}" >> performance-summary.md
          echo "- Load Testing: ${{ needs.load-testing.result }}" >> performance-summary.md
          echo "- Memory Profiling: ${{ needs.memory-profiling.result }}" >> performance-summary.md
          echo "" >> performance-summary.md
          
          echo "## Recommendations" >> performance-summary.md
          echo "- Review benchmark results for performance regressions" >> performance-summary.md
          echo "- Monitor memory usage trends over time" >> performance-summary.md
          echo "- Set up alerts for performance degradation" >> performance-summary.md
          echo "" >> performance-summary.md
          
          echo "Monitoring completed at: $(date)" >> performance-summary.md

      - name: Upload performance summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary
          path: performance-summary.md